{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-01-14T12:56:43.946366Z",
          "iopub.status.busy": "2025-01-14T12:56:43.946038Z",
          "iopub.status.idle": "2025-01-14T12:56:53.108268Z",
          "shell.execute_reply": "2025-01-14T12:56:53.107147Z",
          "shell.execute_reply.started": "2025-01-14T12:56:43.946325Z"
        },
        "id": "BbPNdHK2_mAC",
        "outputId": "ee26f21d-357f-4056-e73f-3e65d1803249",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install grakel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-14T17:40:12.957764Z",
          "iopub.status.busy": "2025-01-14T17:40:12.957439Z",
          "iopub.status.idle": "2025-01-14T17:40:12.982583Z",
          "shell.execute_reply": "2025-01-14T17:40:12.981769Z",
          "shell.execute_reply.started": "2025-01-14T17:40:12.957740Z"
        },
        "id": "BXcbVGGmjjLI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Autoencoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import GINConv\n",
        "from torch_geometric.nn import global_add_pool\n",
        "\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, n_layers, n_nodes):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_nodes = n_nodes\n",
        "\n",
        "        mlp_layers = (\n",
        "            [nn.Linear(latent_dim, hidden_dim)] +\n",
        "            [nn.Linear(hidden_dim, hidden_dim) for i in range(n_layers - 2)]\n",
        "            )\n",
        "        mlp_layers.append(nn.Linear(hidden_dim, 2*n_nodes*(n_nodes-1)//2))\n",
        "\n",
        "        self.mlp = nn.ModuleList(mlp_layers)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x=data.x\n",
        "        for i in range(self.n_layers-1):\n",
        "            x = self.relu(self.mlp[i](x))\n",
        "\n",
        "        x = self.mlp[self.n_layers-1](x)\n",
        "        x = torch.reshape(x, (x.size(0), -1, 2))\n",
        "        x = F.gumbel_softmax(x, tau=1, hard=True)[:, :, 0]\n",
        "\n",
        "        adj = torch.zeros(x.size(0),\n",
        "                          self.n_nodes,\n",
        "                          self.n_nodes,\n",
        "                          device=x.device)\n",
        "\n",
        "        idx = torch.triu_indices(self.n_nodes, self.n_nodes, 1)\n",
        "        adj[:, idx[0], idx[1]] = x\n",
        "        adj = adj + torch.transpose(adj, 1, 2)\n",
        "        return adj\n",
        "class NewDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, n_layers, n_nodes, cond_dim=7):\n",
        "\n",
        "        super(NewDecoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_nodes = n_nodes\n",
        "        self.cond_dim = cond_dim\n",
        "\n",
        "        input_dim = latent_dim + cond_dim if cond_dim else latent_dim\n",
        "\n",
        "        mlp_layers = (\n",
        "            [nn.Linear(input_dim, hidden_dim)] +\n",
        "            [nn.Linear(hidden_dim, hidden_dim) for i in range(n_layers - 2)]\n",
        "        )\n",
        "        mlp_layers.append(nn.Linear(hidden_dim, 2 * n_nodes * (n_nodes - 1) // 2))\n",
        "\n",
        "        self.mlp = nn.ModuleList(mlp_layers)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "\n",
        "\n",
        "        for i in range(cond.size(1)):  # Iterate over features\n",
        "            col_mean = torch.nanmean(cond[:, i])  # Mean for each feature\n",
        "            cond[:, i] = torch.nan_to_num(cond[:, i], nan=col_mean)\n",
        "        min_val = torch.min(cond, dim=0)[0]\n",
        "        max_val = torch.max(cond, dim=0)[0]\n",
        "        cond = (cond - min_val) / (max_val - min_val)\n",
        "\n",
        "\n",
        "\n",
        "        x = torch.cat([x, cond], dim=-1)\n",
        "\n",
        "        # Pass through MLP\n",
        "        for i in range(self.n_layers - 1):\n",
        "            x = self.relu(self.mlp[i](x))\n",
        "        x = self.mlp[self.n_layers - 1](x)\n",
        "\n",
        "        # Reshape and apply Gumbel-Softmax\n",
        "        x = torch.reshape(x, (x.size(0), -1, 2))\n",
        "        x = F.gumbel_softmax(x, tau=1, hard=True)[:, :, 0]\n",
        "\n",
        "        # Create adjacency matrix\n",
        "        adj = torch.zeros(x.size(0), self.n_nodes, self.n_nodes, device=x.device)\n",
        "        idx = torch.triu_indices(self.n_nodes, self.n_nodes, 1)\n",
        "        adj[:, idx[0], idx[1]] = x\n",
        "        adj = adj + torch.transpose(adj, 1, 2)\n",
        "        return adj\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 hidden_dim,\n",
        "                 latent_dim,\n",
        "                 n_layers,\n",
        "                 n_max_nodes,\n",
        "                 dropout=0.4,\n",
        "                 cond_dim=7,\n",
        "                 d_cond=16):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.mlp_adj = nn.Sequential(\n",
        "            nn.Linear(n_max_nodes ** 2, latent_dim),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(latent_dim),\n",
        "            nn.Linear(latent_dim, latent_dim),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(latent_dim),\n",
        "            nn.Linear(latent_dim, input_dim),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GINConv(\n",
        "                            nn.Sequential(\n",
        "                                nn.Linear(input_dim,\n",
        "                                          hidden_dim),\n",
        "                                nn.LeakyReLU(0.2),\n",
        "                                nn.BatchNorm1d(hidden_dim),\n",
        "                                nn.Linear(hidden_dim, hidden_dim),\n",
        "                                nn.LeakyReLU(0.2))\n",
        "                            ))\n",
        "        for layer in range(n_layers-1):\n",
        "            self.convs.append(GINConv(\n",
        "                                nn.Sequential(\n",
        "                                    nn.Linear(hidden_dim,\n",
        "                                              hidden_dim),\n",
        "                                    nn.LeakyReLU(0.2),\n",
        "                                    nn.BatchNorm1d(hidden_dim),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim),\n",
        "                                    nn.LeakyReLU(0.2))\n",
        "                                    ))\n",
        "        self.cond_mlp = nn.Sequential(\n",
        "            nn.Linear(cond_dim, d_cond),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_cond, d_cond),\n",
        "        )\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.concat_conv = nn.Sequential(\n",
        "            nn.Linear(hidden_dim + cond_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "    def forward(self, data):\n",
        "        edge_index = data.edge_index\n",
        "        x = data.x\n",
        "        #A = data.A\n",
        "        cond=data.stats\n",
        "        for i in range(cond.size(1)):  # Iterate over features\n",
        "            col_mean = torch.nanmean(cond[:, i])  # Mean for each feature\n",
        "            cond[:, i] = torch.nan_to_num(cond[:, i], nan=col_mean)\n",
        "        min_val = torch.min(cond, dim=0)[0]\n",
        "        max_val = torch.max(cond, dim=0)[0]\n",
        "        cond = (cond - min_val) / (max_val - min_val)\n",
        "        #cond=self.cond_mlp(cond)\n",
        "        # Flatten symmetric matrix A to vector\n",
        "        #A = A.view(A.size(0), -1)\n",
        "        #vec_A = self.mlp_adj(A)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "        x = global_add_pool(x, data.batch)\n",
        "        #x = torch.cat((x, vec_A), dim=1)\n",
        "        x = torch.cat((x, cond), dim=1)\n",
        "\n",
        "        x = self.concat_conv(x)\n",
        "        out = self.bn(x)\n",
        "        out = self.fc(x)\n",
        "        return out\n",
        "\n",
        "# Variational Autoencoder\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 hidden_dim_enc,\n",
        "                 hidden_dim_dec,\n",
        "                 latent_dim,\n",
        "                 n_layers_enc,\n",
        "                 n_layers_dec,\n",
        "                 n_max_nodes):\n",
        "        super(VariationalAutoEncoder, self).__init__()\n",
        "        self.n_max_nodes = n_max_nodes\n",
        "        self.input_dim = input_dim\n",
        "        self.encoder = GIN(input_dim,\n",
        "                           hidden_dim_enc,\n",
        "                           hidden_dim_enc,\n",
        "                           n_layers_enc,\n",
        "                           n_max_nodes)\n",
        "        self.fc_mu = nn.Linear(hidden_dim_enc, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim_enc, latent_dim)\n",
        "        self.decoder = NewDecoder(latent_dim,\n",
        "                               hidden_dim_dec,\n",
        "                               n_layers_dec,\n",
        "                               n_max_nodes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x_g = self.encoder(data)\n",
        "        mu = self.fc_mu(x_g)\n",
        "        logvar = self.fc_logvar(x_g)\n",
        "        x_g = self.reparameterize(mu, logvar)\n",
        "        adj = self.decoder(x_g,data.stats)\n",
        "        return adj\n",
        "\n",
        "    def encode(self, data):\n",
        "        x_g = self.encoder(data)\n",
        "        mu = self.fc_mu(x_g)\n",
        "        logvar = self.fc_logvar(x_g)\n",
        "        x_g = self.reparameterize(mu, logvar)\n",
        "        return x_g\n",
        "\n",
        "    def reparameterize(self, mu, logvar, eps_scale=1.):\n",
        "        if self.training:\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.randn_like(std) * eps_scale\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, mu, logvar,cond):\n",
        "        x_g = self.reparameterize(mu, logvar)\n",
        "        adj = self.decoder(x_g,cond)\n",
        "        return adj\n",
        "\n",
        "    def decode_mu(self, mu, cond,threshold=0.5):\n",
        "        adj_logits = self.decoder(mu,cond)\n",
        "        adj_probs = torch.sigmoid(adj_logits)\n",
        "        adj = adj_probs * (1 - torch.eye(adj_probs.size(-2), adj_probs.size(-1), device=adj_probs.device))\n",
        "        # Thresholding to get binary adjacency\n",
        "        adj_binary = (adj > threshold).float()\n",
        "        return adj_binary\n",
        "\n",
        "    def loss_function(self, data, beta=0.01):\n",
        "        x_g = self.encoder(data)\n",
        "        mu = self.fc_mu(x_g)\n",
        "        logvar = self.fc_logvar(x_g)\n",
        "        x_g = self.reparameterize(mu, logvar)\n",
        "        adj = self.decoder(x_g,data.stats)\n",
        "        #recon = F.binary_cross_entropy_with_logits(adj, data.A, reduction='sum')\n",
        "        recon =  F.l1_loss(adj, data.A, reduction='sum')\n",
        "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        loss = recon + beta*kld\n",
        "\n",
        "        return loss, recon, kld\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-14T17:40:22.125807Z",
          "iopub.status.busy": "2025-01-14T17:40:22.125480Z",
          "iopub.status.idle": "2025-01-14T17:40:22.144438Z",
          "shell.execute_reply": "2025-01-14T17:40:22.143591Z",
          "shell.execute_reply.started": "2025-01-14T17:40:22.125784Z"
        },
        "id": "HXL7hW3_jjLP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Denoise model\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    \"\"\"\n",
        "    Extract values from a tensor `a` at positions defined by `t`.\n",
        "\n",
        "    Parameters:\n",
        "        a (torch.Tensor): The tensor to extract from.\n",
        "        t (torch.Tensor): Indices tensor.\n",
        "        x_shape (tuple): Target shape for the output.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Extracted values reshaped to match `x_shape`.\n",
        "    \"\"\"\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "\n",
        "def q_sample(x_start, t, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod,\n",
        "             noise=None):\n",
        "    \"\"\"\n",
        "    Forward diffusion using a predefined property.\n",
        "\n",
        "    Parameters:\n",
        "        x_start (torch.Tensor): Initial input tensor.\n",
        "        t (torch.Tensor): Time step tensor.\n",
        "        sqrt_alphas_cumprod (torch.Tensor): Cumulative product of alphas.\n",
        "        sqrt_one_minus_alphas_cumprod (torch.Tensor):\n",
        "        C\n",
        "umulative product for noise.\n",
        "        noise (torch.Tensor, optional): Random noise tensor. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Noisy input after forward diffusion.\n",
        "    \"\"\"\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
        "    )\n",
        "\n",
        "    output = (sqrt_alphas_cumprod_t * x_start\n",
        "              + sqrt_one_minus_alphas_cumprod_t * noise)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def p_losses(denoise_model, x_start, t, cond, sqrt_alphas_cumprod,\n",
        "             sqrt_one_minus_alphas_cumprod, noise=None, loss_type=\"l1\"):\n",
        "    \"\"\"\n",
        "    Calculate the loss for denoising.\n",
        "\n",
        "    Parameters:\n",
        "        denoise_model (nn.Module): The denoising model.\n",
        "        x_start (torch.Tensor): Original data.\n",
        "        t (torch.Tensor): Time steps.\n",
        "        cond (torch.Tensor): Conditioning information.\n",
        "        sqrt_alphas_cumprod (torch.Tensor): Cumulative product of alphas.\n",
        "        sqrt_one_minus_alphas_cumprod (torch.Tensor): Noise term.\n",
        "        noise (torch.Tensor, optional): Random noise. Defaults to None.\n",
        "        loss_type (str): Type of loss. Can be \"l1\", \"l2\", or \"huber\".\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Computed loss.\n",
        "    \"\"\"\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start, t, sqrt_alphas_cumprod,\n",
        "                       sqrt_one_minus_alphas_cumprod, noise=noise)\n",
        "    predicted_noise = denoise_model(x_noisy, t, cond)\n",
        "\n",
        "    if loss_type == 'l1':\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == 'l2':\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Loss type {loss_type} is not implemented.\")\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Module for generating sinusoidal position embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.arange(half_dim, device=device) * -embeddings\n",
        "        embeddings = torch.exp(embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class DenoiseNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Denoising neural network with temporal and conditional embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, n_layers, n_cond, d_cond):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_cond = n_cond\n",
        "\n",
        "\n",
        "        # Conditional embedding network\n",
        "        self.cond_mlp = nn.Sequential(\n",
        "            nn.Linear(self.n_cond, d_cond),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_cond, d_cond),\n",
        "        )\n",
        "\n",
        "        # Time embedding network\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "        # Main network\n",
        "        mlp_layers = (\n",
        "            [nn.Linear(input_dim + d_cond, hidden_dim)] +\n",
        "            [nn.Linear(hidden_dim + d_cond, hidden_dim)\n",
        "             for _ in range(n_layers - 2)]\n",
        "        )\n",
        "        mlp_layers.append(nn.Linear(hidden_dim, input_dim))\n",
        "        self.mlp = nn.ModuleList(mlp_layers)\n",
        "\n",
        "        # Batch normalization\n",
        "        self.bn = nn.ModuleList(\n",
        "            [nn.BatchNorm1d(hidden_dim) for _ in range(n_layers - 1)]\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t, cond):\n",
        "\n",
        "        cond = cond.view(-1, self.n_cond)\n",
        "        for i in range(cond.size(1)):  # Iterate over features\n",
        "            col_mean = torch.nanmean(cond[:, i])  # Mean for each feature\n",
        "            cond[:, i] = torch.nan_to_num(cond[:, i], nan=col_mean)\n",
        "        min_val = torch.min(cond, dim=0)[0]\n",
        "        max_val = torch.max(cond, dim=0)[0]\n",
        "        cond = (cond - min_val) / (max_val - min_val)\n",
        "\n",
        "\n",
        "\n",
        "        cond = self.cond_mlp(cond)\n",
        "        t = self.time_mlp(t)\n",
        "\n",
        "        for i in range(self.n_layers - 1):\n",
        "            x = torch.cat((x, cond), dim=1)\n",
        "            x = self.relu(self.mlp[i](x)) + t\n",
        "            x = self.bn[i](x)\n",
        "\n",
        "        x = self.mlp[self.n_layers - 1](x)\n",
        "        return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, cond, t_index, betas):\n",
        "    \"\"\"\n",
        "    Perform a single reverse diffusion step.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The denoising model.\n",
        "        x (torch.Tensor): Noisy input.\n",
        "        t (torch.Tensor): Time step tensor.\n",
        "        cond (torch.Tensor): Conditional input.\n",
        "        t_index (int): Current timestep index.\n",
        "        betas (torch.Tensor): Beta schedule.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor after one reverse step.\n",
        "    \"\"\"\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "    # sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "\n",
        "    posterior_variance = (\n",
        "        betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "    )\n",
        "\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t, cond) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "\n",
        "    posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "    noise = torch.randn_like(x)\n",
        "    return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, cond, timesteps, betas, shape):\n",
        "    \"\"\"\n",
        "    Perform a full sampling loop over all timesteps.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): Denoising model.\n",
        "        cond (torch.Tensor): Conditional input.\n",
        "        timesteps (int): Number of timesteps.\n",
        "        betas (torch.Tensor): Beta schedule.\n",
        "        shape (tuple): Shape of the output tensor.\n",
        "\n",
        "    Returns:\n",
        "        list[torch.Tensor]: List of sampled images at each timestep.\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    img = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "\n",
        "    for i in reversed(range(timesteps)):\n",
        "        img = p_sample(\n",
        "            model,\n",
        "            img,\n",
        "            torch.full((shape[0],), i, device=device, dtype=torch.long),\n",
        "            cond,\n",
        "            i,\n",
        "            betas\n",
        "        )\n",
        "        imgs.append(img)\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, cond, latent_dim, timesteps, betas, batch_size):\n",
        "    \"\"\"\n",
        "    Sample images from the model.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): Denoising model.\n",
        "        cond (torch.Tensor): Conditional input.\n",
        "        latent_dim (int): Dimensionality of the latent space.\n",
        "        timesteps (int): Number of timesteps.\n",
        "        betas (torch.Tensor): Beta schedule.\n",
        "        batch_size (int): Number of samples in the batch.\n",
        "\n",
        "    Returns:\n",
        "        list[torch.Tensor]: Generated samples.\n",
        "    \"\"\"\n",
        "    return p_sample_loop(model, cond, timesteps, betas, shape=(batch_size,\n",
        "                                                               latent_dim))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-14T12:56:58.985293Z",
          "iopub.status.busy": "2025-01-14T12:56:58.985065Z",
          "iopub.status.idle": "2025-01-14T12:56:58.989886Z",
          "shell.execute_reply": "2025-01-14T12:56:58.989145Z",
          "shell.execute_reply.started": "2025-01-14T12:56:58.985275Z"
        },
        "id": "2Ed2GYHmjjLS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Extracts feats\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import re\n",
        "\n",
        "random.seed(32)\n",
        "\n",
        "\n",
        "def extract_numbers(text):\n",
        "    # Use regular expression to find integers and floats\n",
        "    numbers = re.findall(r'\\d+\\.\\d+|\\d+', text)\n",
        "    # Convert the extracted numbers to float\n",
        "    return [float(num) for num in numbers]\n",
        "\n",
        "\n",
        "def extract_feats(file):\n",
        "    stats = []\n",
        "    fread = open(file, \"r\")\n",
        "    line = fread.read()\n",
        "    line = line.strip()\n",
        "    stats = extract_numbers(line)\n",
        "    fread.close()\n",
        "    return stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-14T13:15:55.942232Z",
          "iopub.status.busy": "2025-01-14T13:15:55.941921Z",
          "iopub.status.idle": "2025-01-14T13:15:55.973927Z",
          "shell.execute_reply": "2025-01-14T13:15:55.972882Z",
          "shell.execute_reply.started": "2025-01-14T13:15:55.942207Z"
        },
        "id": "I-lsmYg7jjLT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "\n",
        "import os\n",
        "import math\n",
        "import requests\n",
        "import zipfile\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.sparse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import community as community_louvain\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from grakel.utils import graph_from_networkx\n",
        "from grakel.kernels import WeisfeilerLehman, VertexHistogram\n",
        "from tqdm import tqdm\n",
        "import scipy.sparse as sparse\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Extracts feats\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import re\n",
        "\n",
        "random.seed(32)\n",
        "\n",
        "\n",
        "def extract_numbers(text):\n",
        "    # Use regular expression to find integers and floats\n",
        "    numbers = re.findall(r'\\d+\\.\\d+|\\d+', text)\n",
        "    # Convert the extracted numbers to float\n",
        "    return [float(num) for num in numbers]\n",
        "\n",
        "\n",
        "def extract_feats(file):\n",
        "    stats = []\n",
        "    fread = open(file, \"r\")\n",
        "    line = fread.read()\n",
        "    line = line.strip()\n",
        "    stats = extract_numbers(line)\n",
        "    fread.close()\n",
        "    return stats\n",
        "\n",
        "\n",
        "def load_data(repo_url, download_dir):\n",
        "    try:\n",
        "        zip_url = f\"{repo_url}/archive/refs/heads/main.zip\"  # Update branch if not \"main\"\n",
        "        response = requests.get(zip_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Save zip file\n",
        "        zip_path = os.path.join(download_dir, \"repo.zip\")\n",
        "        with open(zip_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Extract zip file\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(download_dir)\n",
        "\n",
        "        # Clean up zip file\n",
        "        os.remove(zip_path)\n",
        "\n",
        "        # Locate the folder that was created after extracting the repo\n",
        "        repo_folder_name = os.path.basename(repo_url) + \"-main\"\n",
        "        repo_folder_path = os.path.join(download_dir, repo_folder_name)\n",
        "\n",
        "        # Path to the new DATA folder\n",
        "        data_folder_path = os.path.join(download_dir, \"data\")\n",
        "        os.makedirs(data_folder_path, exist_ok=True)\n",
        "\n",
        "        # Path to the data.zip inside the extracted repo folder\n",
        "        data_zip_path = os.path.join(repo_folder_path, \"data.zip\")\n",
        "\n",
        "        # Extract data.zip into the DATA folder\n",
        "        with zipfile.ZipFile(data_zip_path, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(data_folder_path)\n",
        "\n",
        "        # Remove the repo folder after extracting data.zip\n",
        "        shutil.rmtree(repo_folder_path)\n",
        "\n",
        "        print(f\"Repository and data extracted successfully. data folder created at {data_folder_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "def preprocess_dataset(dataset, n_max_nodes, spectral_emb_dim):\n",
        "    data_lst = []\n",
        "\n",
        "    if dataset == 'test':\n",
        "        desc_file = './data/data/' + dataset + '/test.txt'\n",
        "        fr = open(desc_file, \"r\")\n",
        "        for line in fr:\n",
        "            line = line.strip()\n",
        "            tokens = line.split(\",\")\n",
        "            graph_id = tokens[0]\n",
        "            desc = tokens[1:]\n",
        "            desc = \"\".join(desc)\n",
        "            feats_stats = extract_numbers(desc)\n",
        "            feats_stats = torch.FloatTensor(feats_stats).unsqueeze(0)\n",
        "            data_lst.append(Data(stats=feats_stats, filename=graph_id))\n",
        "        fr.close()\n",
        "\n",
        "    else:\n",
        "        graph_path = './data/data/' + dataset + '/graph'\n",
        "        desc_path = './data/data/' + dataset + '/description'\n",
        "\n",
        "        # Traverse through all the graphs in the folder\n",
        "        files = [f for f in os.listdir(graph_path)]\n",
        "        for fileread in tqdm(files):\n",
        "            tokens = fileread.split(\"/\")\n",
        "            idx = tokens[-1].find(\".\")\n",
        "            filen = tokens[-1][:idx]\n",
        "            extension = tokens[-1][idx + 1:]\n",
        "            fread = os.path.join(graph_path, fileread)\n",
        "            fstats = os.path.join(desc_path, filen + \".txt\")\n",
        "\n",
        "            # Load dataset to networkx\n",
        "            if extension == \"graphml\":\n",
        "                G = nx.read_graphml(fread)\n",
        "                G = nx.convert_node_labels_to_integers(G, ordering=\"sorted\")\n",
        "            else:\n",
        "                G = nx.read_edgelist(fread)\n",
        "\n",
        "            # Use BFS for adjacency matrix creation\n",
        "            CGs = [G.subgraph(c) for c in nx.connected_components(G)]\n",
        "            CGs = sorted(CGs, key=lambda x: x.number_of_nodes(), reverse=True)\n",
        "\n",
        "            node_list_bfs = []\n",
        "            for ii in range(len(CGs)):\n",
        "                node_degree_list = [(n, d) for n, d in CGs[ii].degree()]\n",
        "                degree_sequence = sorted(node_degree_list, key=lambda tt: tt[1], reverse=True)\n",
        "                bfs_tree = nx.bfs_tree(CGs[ii], source=degree_sequence[0][0])\n",
        "                node_list_bfs += list(bfs_tree.nodes())\n",
        "\n",
        "            adj_bfs = nx.to_numpy_array(G, nodelist=node_list_bfs)\n",
        "            adj = torch.from_numpy(adj_bfs).float()\n",
        "            diags = np.sum(adj_bfs, axis=0)\n",
        "            diags = np.squeeze(np.asarray(diags))\n",
        "            D = sparse.diags(diags).toarray()\n",
        "            L = D - adj_bfs\n",
        "            with np.errstate(divide=\"ignore\"):\n",
        "                diags_sqrt = 1.0 / np.sqrt(diags)\n",
        "            diags_sqrt[np.isinf(diags_sqrt)] = 0\n",
        "            DH = sparse.diags(diags).toarray()\n",
        "            L = np.linalg.multi_dot((DH, L, DH))\n",
        "            L = torch.from_numpy(L).float()\n",
        "            eigval, eigvecs = torch.linalg.eigh(L)\n",
        "            eigval = torch.real(eigval)\n",
        "            eigvecs = torch.real(eigvecs)\n",
        "            idx = torch.argsort(eigval)\n",
        "            eigvecs = eigvecs[:, idx]\n",
        "\n",
        "            edge_index = torch.nonzero(adj).t()\n",
        "\n",
        "            size_diff = n_max_nodes - G.number_of_nodes()\n",
        "            x = torch.zeros(G.number_of_nodes(), spectral_emb_dim + 1)\n",
        "            x[:, 0] = torch.mm(adj, torch.ones(G.number_of_nodes(), 1))[:, 0] / (n_max_nodes - 1)\n",
        "            mn = min(G.number_of_nodes(), spectral_emb_dim)\n",
        "            mn += 1\n",
        "            x[:, 1:mn] = eigvecs[:, :spectral_emb_dim]\n",
        "            adj = F.pad(adj, [0, size_diff, 0, size_diff])\n",
        "            adj = adj.unsqueeze(0)\n",
        "\n",
        "            feats_stats = extract_feats(fstats)\n",
        "            feats_stats = torch.FloatTensor(feats_stats).unsqueeze(0)\n",
        "\n",
        "            data_lst.append(Data(x=x, edge_index=edge_index, A=adj, stats=feats_stats, filename=filen))\n",
        "\n",
        "    return data_lst\n",
        "\n",
        "def construct_nx_from_adj(adj):\n",
        "    G = nx.from_numpy_array(adj, create_using=nx.Graph)\n",
        "    to_remove = []\n",
        "    for node in G.nodes():\n",
        "        if G.degree(node) == 0:\n",
        "            to_remove.append(node)\n",
        "    G.remove_nodes_from(to_remove)\n",
        "    return G\n",
        "\n",
        "\n",
        "def handle_nan(x):\n",
        "    if math.isnan(x):\n",
        "        return float(-100)\n",
        "    return x\n",
        "\n",
        "\n",
        "def masked_instance_norm2D(x: torch.Tensor,\n",
        "                           mask: torch.Tensor,\n",
        "                           eps: float = 1e-5):\n",
        "    \"\"\"\n",
        "    x: [batch_size (N), num_objects (L), num_objects (L), features(C)]\n",
        "    mask: [batch_size (N), num_objects (L), num_objects (L), 1]\n",
        "    \"\"\"\n",
        "    mask = mask.view(x.size(0), x.size(1), x.size(2), 1).expand_as(x)\n",
        "    # (N,C)\n",
        "    mean = (torch.sum(x * mask, dim=[1, 2]) / torch.sum(mask, dim=[1, 2]))\n",
        "    # (N,L,L,C)\n",
        "    var_term = ((x - mean.unsqueeze(1).unsqueeze(1).expand_as(x)) * mask)**2\n",
        "    # (N,C)\n",
        "    var = (torch.sum(var_term, dim=[1, 2]) / torch.sum(mask, dim=[1, 2]))\n",
        "    mean = mean.unsqueeze(1).unsqueeze(1).expand_as(x)  # (N, L, L, C)\n",
        "    var = var.unsqueeze(1).unsqueeze(1).expand_as(x)    # (N, L, L, C)\n",
        "    instance_norm = (x - mean) / torch.sqrt(var + eps)   # (N, L, L, C)\n",
        "    instance_norm = instance_norm * mask\n",
        "    return instance_norm\n",
        "\n",
        "\n",
        "def masked_layer_norm2D(x: torch.Tensor,\n",
        "                        mask: torch.Tensor,\n",
        "                        eps: float = 1e-5):\n",
        "    \"\"\"\n",
        "    x: [batch_size (N), num_objects (L), num_objects (L), features(C)]\n",
        "    mask: [batch_size (N), num_objects (L), num_objects (L), 1]\n",
        "    \"\"\"\n",
        "    mask = mask.view(x.size(0), x.size(1), x.size(2), 1).expand_as(x)\n",
        "    # (N)\n",
        "    mean = torch.sum(x * mask, dim=[3, 2, 1]) / torch.sum(mask, dim=[3, 2, 1])\n",
        "    # (N,L,L,C)\n",
        "    var_term = ((x - mean.view(-1, 1, 1, 1).expand_as(x)) * mask)**2\n",
        "    var = (torch.sum(var_term, dim=[3, 2, 1]) / torch.sum(mask, dim=[3, 2, 1]))\n",
        "    # (N)\n",
        "    mean = mean.view(-1, 1, 1, 1).expand_as(x)  # (N, L, L, C)\n",
        "    var = var.view(-1, 1, 1, 1).expand_as(x)    # (N, L, L, C)\n",
        "    layer_norm = (x - mean) / torch.sqrt(var + eps)   # (N, L, L, C)\n",
        "    layer_norm = layer_norm * mask\n",
        "    return layer_norm\n",
        "\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s)\n",
        "                               * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
        "\n",
        "\n",
        "def graph_features(adj_matrix):\n",
        "    \"\"\"\n",
        "    Compute features of a graph from the adjacency matrix, removing trailing zero blocks.\n",
        "\n",
        "    Parameters:\n",
        "        adj_matrix (numpy.ndarray): Adjacency matrix of the graph.\n",
        "\n",
        "    Returns:\n",
        "        list: List of graph features.\n",
        "    \"\"\"\n",
        "    G= construct_nx_from_adj(adj_matrix)\n",
        "\n",
        "    # Compute graph features\n",
        "    num_nodes = G.number_of_nodes()  # Number of nodes\n",
        "    num_edges = G.number_of_edges()  # Number of edges\n",
        "    avg_degree = sum(dict(G.degree()).values()) / num_nodes if num_nodes > 0 else 0  # Average degree\n",
        "    has_triangle = any(len(cycle) == 3 for cycle in nx.cycle_basis(G))  # Check for triangles\n",
        "    global_clustering = nx.transitivity(G)  # Global clustering coefficient\n",
        "    k_core = max(nx.core_number(G).values()) if num_nodes > 0 else 0  # Maximum k-core value\n",
        "    communities = list(nx.connected_components(G))  # Connected components as communities\n",
        "    num_communities = len(communities)  # Number of communities\n",
        "    triangle_counts = nx.triangles(G)  # Count of triangles for each node\n",
        "    num_triangles = sum(triangle_counts.values()) // 3  # Total triangles (each counted 3 times)\n",
        "\n",
        "    # Return the features as a list\n",
        "    return [num_nodes, num_edges, avg_degree, num_triangles, global_clustering, k_core, num_communities]\n",
        "\n",
        "def fast_graph_features(data):\n",
        "    \"\"\"\n",
        "    Compute fast-to-compute features of a graph from the given Data object.\n",
        "\n",
        "    Parameters:\n",
        "        data (Data): PyTorch Geometric Data object with attributes:\n",
        "            - edge_index: Edge list as a tensor.\n",
        "            - x: Node features (optional).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of fast-to-compute graph features.\n",
        "    \"\"\"\n",
        "    edge_index = data.edge_index  # Edge list\n",
        "    num_nodes = data.num_nodes\n",
        "\n",
        "    # Compute number of edges\n",
        "    num_edges = edge_index.size(1)\n",
        "\n",
        "    # Compute degree and average degree\n",
        "    degrees = torch.bincount(edge_index[0], minlength=num_nodes).float()  # Convert to float\n",
        "    avg_degree = degrees.mean()\n",
        "\n",
        "    # Prepare the feature vector\n",
        "    features = torch.tensor(\n",
        "        [\n",
        "            num_nodes,\n",
        "            num_edges,\n",
        "            avg_degree.item(),\n",
        "        ],\n",
        "        device=edge_index.device,\n",
        "        dtype=torch.float,\n",
        "    )\n",
        "\n",
        "    return features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-14T17:40:33.227470Z",
          "iopub.status.busy": "2025-01-14T17:40:33.227131Z",
          "iopub.status.idle": "2025-01-14T17:40:33.232475Z",
          "shell.execute_reply": "2025-01-14T17:40:33.231591Z",
          "shell.execute_reply.started": "2025-01-14T17:40:33.227443Z"
        },
        "id": "QCcZxLaZjjLW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import scipy as sp\n",
        "import pickle\n",
        "\n",
        "import shutil\n",
        "import csv\n",
        "import ast\n",
        "\n",
        "import scipy.sparse as sparse\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "np.random.seed(13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-01-14T12:56:59.921845Z",
          "iopub.status.busy": "2025-01-14T12:56:59.921556Z",
          "iopub.status.idle": "2025-01-14T12:57:04.766538Z",
          "shell.execute_reply": "2025-01-14T12:57:04.765280Z",
          "shell.execute_reply.started": "2025-01-14T12:56:59.921817Z"
        },
        "id": "neAK6DnojjLX",
        "outputId": "f205bcff-ce96-4ba6-8142-5923d9d0850e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Loading Data\n",
        "# Here, we load the data from the repository.\n",
        "\n",
        "repo_url = \"https://github.com/chris-mrn/Data_Altegrad\"\n",
        "download_dir = os.getcwd()\n",
        "\n",
        "# Load the data\n",
        "load_data(repo_url, download_dir)\n",
        "print(\"Data loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-14T17:40:39.698745Z",
          "iopub.status.busy": "2025-01-14T17:40:39.698458Z",
          "iopub.status.idle": "2025-01-14T17:40:39.703911Z",
          "shell.execute_reply": "2025-01-14T17:40:39.703119Z",
          "shell.execute_reply.started": "2025-01-14T17:40:39.698723Z"
        },
        "id": "6jWBWfIyjjLX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Cell 3: Set up argument parser for model configuration\n",
        "# This block sets up the arguments for configuring the model using SimpleNamespace.\n",
        "args = SimpleNamespace(**{\n",
        "    'lr': 1e-3,  # Learning rate for the optimizer (default: 0.001)\n",
        "    'dropout': 0.0,  # Dropout rate (default: 0.0)\n",
        "    'batch_size': 256,  # Batch size for training (default: 256)\n",
        "    'epochs_autoencoder': 600,  # Training epochs for autoencoder (default: 200)\n",
        "    'hidden_dim_encoder': 64,  # Hidden dimension size for encoder (default: 64)\n",
        "    'hidden_dim_decoder': 512,  # Hidden dimension size for decoder (default: 256)\n",
        "    'latent_dim': 32,  # Latent space dimensionality (default: 32)\n",
        "    'n_max_nodes': 50,  # Maximum number of nodes (default: 50)\n",
        "    'n_layers_encoder': 2,  # Number of encoder layers (default: 2)\n",
        "    'n_layers_decoder': 3,  # Number of decoder layers (default: 3)\n",
        "    'spectral_emb_dim': 10,  # Dimensionality of spectral embeddings (default: 10)\n",
        "    'epochs_denoise': 300,  # Training epochs for denoising model (default: 100)\n",
        "    'timesteps': 500,  # Number of timesteps for diffusion (default: 500)\n",
        "    'hidden_dim_denoise': 512,  # Hidden dimension size for denoising model (default: 512)\n",
        "    'n_layers_denoise': 3,  # Number of denoising model layers (default: 3)\n",
        "    'train_autoencoder': True,  # Flag to toggle autoencoder training (default: enabled)\n",
        "    'train_denoiser': True,  # Flag to toggle denoiser training (default: enabled)\n",
        "    'dim_condition': 128,  # Dimensionality of conditioning vectors (default: 128)\n",
        "    'n_condition': 7,  # Number of distinct condition properties (default: 7)\n",
        "})\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-01-14T12:57:04.778281Z",
          "iopub.status.busy": "2025-01-14T12:57:04.777979Z",
          "iopub.status.idle": "2025-01-14T12:57:45.626278Z",
          "shell.execute_reply": "2025-01-14T12:57:45.625387Z",
          "shell.execute_reply.started": "2025-01-14T12:57:04.778251Z"
        },
        "id": "0MHr7aqvjjLY",
        "outputId": "fbe46280-e8ed-436f-b355-a88288709329",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# preprocess train data, validation data and test data. Only once for the first time that you run the code. Then the appropriate .pt files will be saved and loaded.\n",
        "trainset = preprocess_dataset(\"train\", args.n_max_nodes, args.spectral_emb_dim)\n",
        "validset = preprocess_dataset(\"valid\", args.n_max_nodes, args.spectral_emb_dim)\n",
        "testset = preprocess_dataset(\"test\", args.n_max_nodes, args.spectral_emb_dim)\n",
        "\n",
        "\n",
        "# initialize data loaders\n",
        "train_loader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validset, batch_size=args.batch_size, shuffle=False)\n",
        "test_loader = DataLoader(testset, batch_size=args.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "EJS3fW2l9a7r",
        "outputId": "730d4903-01ae-4594-d6b7-68b9033d7032"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_graph(data):\n",
        "    # Assuming `data` is a single graph from the trainset\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    node_features = data.x.cpu().numpy()\n",
        "\n",
        "    # Create a NetworkX graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with features\n",
        "    num_nodes = node_features.shape[0]\n",
        "    for i in range(num_nodes):\n",
        "        G.add_node(i, feature=node_features[i])\n",
        "\n",
        "    # Add edges\n",
        "    for edge in edge_index.T:  # Transpose to get pairs of edges\n",
        "        G.add_edge(edge[0], edge[1])\n",
        "\n",
        "    # Plot the graph\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    pos = nx.spring_layout(G)  # You can choose other layouts like `circular_layout`, etc.\n",
        "    nx.draw(G, pos, with_labels=True, node_size=500, node_color='skyblue', font_size=12, font_weight='bold', edge_color='gray')\n",
        "    plt.title(\"Graph Visualization\")\n",
        "    plt.show()\n",
        "\n",
        "# Example: Plot the first graph in the trainset\n",
        "graph_data = trainset[0]  # Assuming trainset is a list of graphs\n",
        "plot_graph(graph_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "FvGxVEvY96p0",
        "outputId": "81d7c063-faf0-41c7-916c-4cc7746d4dc5"
      },
      "outputs": [],
      "source": [
        "graph_data = trainset[100]  # Assuming trainset is a list of graphs\n",
        "plot_graph(graph_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UMvb3rfkBe_I",
        "outputId": "d7b77369-f31e-4236-96d0-a92f0467c31f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize lists to store losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_recon_losses = []\n",
        "val_recon_losses = []\n",
        "train_kld_losses = []\n",
        "val_kld_losses = []\n",
        "\n",
        "# Initialize VGAE model and optimizer\n",
        "autoencoder = VariationalAutoEncoder(args.spectral_emb_dim + 1,\n",
        "                                     args.hidden_dim_encoder,\n",
        "                                     args.hidden_dim_decoder,\n",
        "                                     args.latent_dim,\n",
        "                                     args.n_layers_encoder,\n",
        "                                     args.n_layers_decoder,\n",
        "                                     args.n_max_nodes).to(device)\n",
        "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=args.lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)\n",
        "\n",
        "# Train VGAE model\n",
        "if args.train_autoencoder:\n",
        "    best_val_loss = np.inf\n",
        "    for epoch in range(1, args.epochs_autoencoder + 1):\n",
        "        autoencoder.train()\n",
        "        train_loss_all = 0\n",
        "        train_count = 0\n",
        "        train_loss_all_recon = 0\n",
        "        train_loss_all_kld = 0\n",
        "        cnt_train = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Check for NaN in the relevant tensors inside the data object\n",
        "            if torch.isnan(data.x).any() or torch.isnan(data.edge_index).any():\n",
        "                print(\"NaN detected in data!\")\n",
        "                continue\n",
        "\n",
        "            loss, recon, kld = autoencoder.loss_function(data)\n",
        "\n",
        "            # Check for NaN in loss\n",
        "            if torch.isnan(loss).any() or torch.isnan(recon).any() or torch.isnan(kld).any():\n",
        "                print(f\"NaN detected in loss (Epoch {epoch})\")\n",
        "                continue\n",
        "\n",
        "            train_loss_all_recon += recon.item()\n",
        "            train_loss_all_kld += kld.item()\n",
        "            cnt_train += 1\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to avoid explosion\n",
        "            torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), max_norm=1.0)\n",
        "\n",
        "            train_loss_all += loss.item()\n",
        "            train_count += torch.max(data.batch) + 1\n",
        "            optimizer.step()\n",
        "\n",
        "        autoencoder.eval()\n",
        "        val_loss_all = 0\n",
        "        val_count = 0\n",
        "        cnt_val = 0\n",
        "        val_loss_all_recon = 0\n",
        "        val_loss_all_kld = 0\n",
        "\n",
        "        for data in val_loader:\n",
        "            data = data.to(device)\n",
        "            loss, recon, kld = autoencoder.loss_function(data)\n",
        "            val_loss_all_recon += recon.item()\n",
        "            val_loss_all_kld += kld.item()\n",
        "            val_loss_all += loss.item()\n",
        "            cnt_val += 1\n",
        "            val_count += torch.max(data.batch) + 1\n",
        "\n",
        "        # Store the losses for plotting\n",
        "        train_losses.append(train_loss_all / cnt_train)\n",
        "        val_losses.append(val_loss_all / cnt_val)\n",
        "        train_recon_losses.append(train_loss_all_recon / cnt_train)\n",
        "        val_recon_losses.append(val_loss_all_recon / cnt_val)\n",
        "        train_kld_losses.append(train_loss_all_kld / cnt_train)\n",
        "        val_kld_losses.append(val_loss_all_kld / cnt_val)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            dt_t = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "            print(f'{dt_t}',\n",
        "                  f'Epoch: {epoch:04d}',\n",
        "                  f'T_l: {train_loss_all/cnt_train:.5f}',\n",
        "                  f'T_rec_l: {train_loss_all_recon/cnt_train:.2f}',\n",
        "                  f'T_KLD_l: {train_loss_all_kld/cnt_train:.2f}',\n",
        "                  f'V_l: {val_loss_all/cnt_val:.5f}',\n",
        "                  f'V_rec_l: {val_loss_all_recon/cnt_val:.2f}',\n",
        "                  f'V_KLD_l: {val_loss_all_kld/cnt_val:.2f}')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if best_val_loss >= val_loss_all:\n",
        "            best_val_loss = val_loss_all\n",
        "            torch.save({\n",
        "                'state_dict': autoencoder.state_dict(),\n",
        "                'optimizer' : optimizer.state_dict(),\n",
        "            }, 'autoencoder.pth.tar')\n",
        "\n",
        "    # After training is complete, plot and save the loss curves\n",
        "    # Plot total loss\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, args.epochs_autoencoder + 1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, args.epochs_autoencoder + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Train vs Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('loss_plot.png')  # Save plot as a PNG file\n",
        "\n",
        "    # Plot reconstruction loss\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, args.epochs_autoencoder + 1), train_recon_losses, label='Train Reconstruction Loss')\n",
        "    plt.plot(range(1, args.epochs_autoencoder + 1), val_recon_losses, label='Validation Reconstruction Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Reconstruction Loss')\n",
        "    plt.title('Train vs Validation Reconstruction Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('recon_loss_plot.png')  # Save plot as a PNG file\n",
        "\n",
        "    # Plot KLD loss\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, args.epochs_autoencoder + 1), train_kld_losses, label='Train KLD Loss')\n",
        "    plt.plot(range(1, args.epochs_autoencoder + 1), val_kld_losses, label='Validation KLD Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('KLD Loss')\n",
        "    plt.title('Train vs Validation KLD Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('kld_loss_plot.png')  # Save plot as a PNG file\n",
        "\n",
        "else:\n",
        "    checkpoint = torch.load('autoencoder.pth.tar')\n",
        "    autoencoder.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "autoencoder.eval()\n",
        "\n",
        "# define beta schedule\n",
        "betas = linear_beta_schedule(timesteps=args.timesteps)\n",
        "\n",
        "# define alphas\n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "\n",
        "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "# initialize denoising model\n",
        "denoise_model = DenoiseNN(input_dim=args.latent_dim, hidden_dim=args.hidden_dim_denoise, n_layers=args.n_layers_denoise, n_cond=args.n_condition, d_cond=args.dim_condition).to(device)\n",
        "# de-hashtag if two GPU are accesible\n",
        "# denoise_model = nn.DataParallel(denoise_model, device_ids=[0, 1])\n",
        "\n",
        "optimizer = torch.optim.Adam(denoise_model.parameters(), lr=args.lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)\n",
        "\n",
        "# Initialize loss tracking lists\n",
        "train_losses_denoiser = []\n",
        "val_losses_denoiser = []\n",
        "\n",
        "if args.train_denoiser:\n",
        "    best_val_loss = np.inf\n",
        "    for epoch in range(1, args.epochs_denoise + 1):\n",
        "        denoise_model.train()\n",
        "        train_loss_all = 0\n",
        "        train_count = 0\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if torch.isnan(data.x).any() or torch.isnan(data.edge_index).any():\n",
        "                print(\"NaN detected in data!\")\n",
        "\n",
        "            x_g = autoencoder.encode(data)\n",
        "            t = torch.randint(0, args.timesteps, (x_g.size(0),), device=device).long()\n",
        "            loss = p_losses(denoise_model, x_g, t, data.stats, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, loss_type=\"huber\")\n",
        "\n",
        "            if torch.isnan(loss).any():\n",
        "                print(f\"NaN detected in loss (Epoch {epoch})\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(denoise_model.parameters(), max_norm=1.0)\n",
        "\n",
        "            train_loss_all += x_g.size(0) * loss.item()\n",
        "            train_count += x_g.size(0)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation loss\n",
        "        denoise_model.eval()\n",
        "        val_loss_all = 0\n",
        "        val_count = 0\n",
        "        for data in val_loader:\n",
        "            data = data.to(device)\n",
        "            x_g = autoencoder.encode(data)\n",
        "            t = torch.randint(0, args.timesteps, (x_g.size(0),), device=device).long()\n",
        "            loss = p_losses(denoise_model, x_g, t, data.stats, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod, loss_type=\"huber\")\n",
        "            val_loss_all += x_g.size(0) * loss.item()\n",
        "            val_count += x_g.size(0)\n",
        "\n",
        "        # Store losses for plotting\n",
        "        train_losses_denoiser.append(train_loss_all / train_count)\n",
        "        val_losses_denoiser.append(val_loss_all / val_count)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            dt_t = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "            print(f'{dt_t}, Epoch: {epoch:04d}, Train Loss: {train_loss_all/train_count:.5f}, Val Loss: {val_loss_all/val_count:.5f}')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if best_val_loss >= val_loss_all:\n",
        "            best_val_loss = val_loss_all\n",
        "            torch.save({\n",
        "                'state_dict': denoise_model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "            }, 'denoise_model.pth.tar')\n",
        "\n",
        "    # Plotting the losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, args.epochs_denoise + 1), train_losses_denoiser, label='Train Loss')\n",
        "    plt.plot(range(1, args.epochs_denoise + 1), val_losses_denoiser, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Denoiser Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('denoiser_loss_plot.png')  # Save the plot\n",
        "    plt.show()  # Show the plot\n",
        "\n",
        "else:\n",
        "    checkpoint = torch.load('denoise_model.pth.tar')\n",
        "    denoise_model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "denoise_model.eval()\n",
        "\n",
        "\n",
        "\n",
        "# Save to a CSV file\n",
        "with open(\"output.csv\", \"w\", newline=\"\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"graph_id\", \"edge_list\"])\n",
        "    for data in tqdm(test_loader, desc='Processing test set'):\n",
        "        data = data.to(device)\n",
        "        graph_ids, stats = data.filename, data.stats\n",
        "        samples = sample(denoise_model, stats, latent_dim=args.latent_dim, timesteps=args.timesteps, betas=betas, batch_size=stats.size(0))\n",
        "        adj = autoencoder.decode_mu(samples[-1],stats)\n",
        "\n",
        "        for i, graph_id in enumerate(graph_ids):\n",
        "            stat_x = stats[i].view(-1, args.n_condition).detach().cpu().numpy()\n",
        "            Gs_generated = construct_nx_from_adj(adj[i].detach().cpu().numpy())\n",
        "            edge_list_text = \", \".join([f\"({u}, {v})\" for u, v in Gs_generated.edges()])\n",
        "            writer.writerow([graph_id, edge_list_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-14T17:52:39.408165Z",
          "iopub.status.busy": "2025-01-14T17:52:39.407844Z",
          "iopub.status.idle": "2025-01-14T17:52:45.403981Z",
          "shell.execute_reply": "2025-01-14T17:52:45.402391Z",
          "shell.execute_reply.started": "2025-01-14T17:52:39.408136Z"
        },
        "id": "1cwOMcmBjjLa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save to a CSV file\n",
        "with open(\"output.csv\", \"w\", newline=\"\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"graph_id\", \"edge_list\"])\n",
        "    for data in tqdm(test_loader, desc='Processing test set'):\n",
        "        data = data.to(device)\n",
        "        graph_ids, stats = data.filename, data.stats\n",
        "        samples = sample(denoise_model, stats, latent_dim=args.latent_dim, timesteps=args.timesteps, betas=betas, batch_size=stats.size(0))\n",
        "        adj = autoencoder.decode_mu(samples[-1],data.stats)\n",
        "        for i, graph_id in enumerate(graph_ids):\n",
        "            stat_x = stats[i].view(-1, args.n_condition).detach().cpu().numpy()\n",
        "            Gs_generated = construct_nx_from_adj(adj[i].detach().cpu().numpy())\n",
        "            edge_list_text = \", \".join([f\"({u}, {v})\" for u, v in Gs_generated.edges()])\n",
        "            writer.writerow([graph_id, edge_list_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-01-14T17:52:48.517948Z",
          "iopub.status.busy": "2025-01-14T17:52:48.517668Z",
          "iopub.status.idle": "2025-01-14T17:52:56.125927Z",
          "shell.execute_reply": "2025-01-14T17:52:56.125148Z",
          "shell.execute_reply.started": "2025-01-14T17:52:48.517925Z"
        },
        "id": "7TjfBLFnjjLb",
        "outputId": "f5045483-7edb-4854-ed47-c52f5398c123",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to normalize features\n",
        "def normalize_features(features):\n",
        "    \"\"\"\n",
        "    Normalize a set of features using min-max normalization.\n",
        "\n",
        "    Parameters:\n",
        "        features (numpy.ndarray): Array of features to normalize.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Normalized features.\n",
        "        tuple: Min and max values for each feature (for future scaling if needed).\n",
        "    \"\"\"\n",
        "    min_vals = np.min(features, axis=0)\n",
        "    max_vals = np.max(features, axis=0)\n",
        "    range_vals = max_vals - min_vals\n",
        "\n",
        "    # Avoid division by zero\n",
        "    range_vals[range_vals == 0] = 1\n",
        "\n",
        "    normalized_features = (features - min_vals) / range_vals\n",
        "    return normalized_features, min_vals, max_vals\n",
        "\n",
        "\n",
        "# Compute MAE for the test set\n",
        "np.set_printoptions(suppress=True, precision=2)\n",
        "\n",
        "MAE_test = 0\n",
        "all_gt_stats = []  # Collect all ground truth stats\n",
        "all_pred_stats = []  # Collect all predicted stats\n",
        "\n",
        "for data in tqdm(test_loader, desc='Processing test set'):\n",
        "    data = data.to(device)\n",
        "    graph_ids, stats = data.filename, data.stats\n",
        "    samples = sample(denoise_model, stats, latent_dim=args.latent_dim, timesteps=args.timesteps, betas=betas, batch_size=stats.size(0))\n",
        "    adj = autoencoder.decode_mu(samples[-1],data.stats)\n",
        "\n",
        "    for i, graph_id in enumerate(graph_ids):\n",
        "        graph_features_pred = graph_features(adj[i].detach().cpu().numpy())\n",
        "        if i % 50 == 0:\n",
        "            print('Stats GT:', np.round(stats[i].detach().cpu().numpy(), 2))\n",
        "            print('Stats PR:', np.round(graph_features_pred, 2))\n",
        "\n",
        "        # Collect ground truth and predicted stats\n",
        "        all_gt_stats.append(stats[i].detach().cpu().numpy())\n",
        "        all_pred_stats.append(graph_features_pred)\n",
        "\n",
        "# Convert collected stats to numpy arrays\n",
        "all_gt_stats = np.array(all_gt_stats)\n",
        "all_pred_stats = np.array(all_pred_stats)\n",
        "\n",
        "# Normalize ground truth and predicted stats\n",
        "normalized_gt_stats, min_vals, max_vals = normalize_features(all_gt_stats)\n",
        "normalized_pred_stats = (all_pred_stats - min_vals) / (max_vals - min_vals)\n",
        "normalized_pred_stats[np.isnan(normalized_pred_stats)] = 0  # Handle potential NaNs\n",
        "\n",
        "# Compute normalized MAE\n",
        "MAE_test = np.mean(np.abs(normalized_gt_stats - normalized_pred_stats))\n",
        "print('Normalized MAE test set:', MAE_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYOo5wgKJMZ_",
        "outputId": "56acfa6f-f630-4369-ffe7-ebc98542aa67"
      },
      "outputs": [],
      "source": [
        "n_samples = 30\n",
        "def compute_mae(adj, graph_ids, stats):\n",
        "    features_pred_batch = []\n",
        "    features_gt_batch = []\n",
        "\n",
        "    for i, graph_id in enumerate(graph_ids):\n",
        "        graph_features_pred = graph_features(adj[i].detach().cpu().numpy())\n",
        "        stats_gt = stats[i].detach().cpu().numpy()\n",
        "        features_pred_batch.append(graph_features_pred)\n",
        "        features_gt_batch.append(stats_gt)\n",
        "\n",
        "    features_pred_batch = np.array(features_pred_batch)\n",
        "    features_gt_batch = np.array(features_gt_batch)\n",
        "\n",
        "    # Normalize ground truth features\n",
        "    norm_gt_stats_batch, min_vals, max_vals = normalize_features(features_gt_batch)\n",
        "\n",
        "    # Normalize predicted features\n",
        "    norm_pred_stats_batch = (features_pred_batch - min_vals) / (max_vals - min_vals)\n",
        "    norm_pred_stats_batch[np.isnan(norm_pred_stats_batch)] = 0  # Handle potential NaNs\n",
        "\n",
        "    # Calculate Mean Absolute Error\n",
        "    MAE = np.mean(np.abs(norm_gt_stats_batch - norm_pred_stats_batch))\n",
        "\n",
        "    return MAE\n",
        "def normalize_features(features):\n",
        "    \"\"\"\n",
        "     Normalize a set of features using min-max normalization.\n",
        "\n",
        "    Parameters:\n",
        "        features (numpy.ndarray): Array of features to normalize.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Normalized features.\n",
        "        tuple: Min and max values for each feature (for future scaling if needed).\n",
        "    \"\"\"\n",
        "    min_vals = np.min(features, axis=0)\n",
        "    max_vals = np.max(features, axis=0)\n",
        "    range_vals = max_vals - min_vals\n",
        "\n",
        "    # Avoid division by zero\n",
        "    range_vals[range_vals == 0] = 1\n",
        "\n",
        "    normalized_features = (features - min_vals) / range_vals\n",
        "    return normalized_features, min_vals, max_vals\n",
        "\n",
        "with open(\"output.csv\", \"w\", newline=\"\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"graph_id\", \"edge_list\"])\n",
        "    for data in tqdm(test_loader, desc='Processing test set'):\n",
        "        data = data.to(device)\n",
        "        graph_ids, stats = data.filename, data.stats\n",
        "        best_MAE = 1\n",
        "        for k in range(n_samples):\n",
        "            samples = sample(denoise_model, stats, latent_dim=args.latent_dim, timesteps=args.timesteps, betas=betas, batch_size=stats.size(0))\n",
        "            adj = autoencoder.decode_mu(samples[-1], stats)\n",
        "            mae = compute_mae(adj, graph_ids, stats)\n",
        "            if mae < best_MAE:\n",
        "                best_MAE = mae\n",
        "                best_samples = samples\n",
        "        adj = autoencoder.decode_mu(best_samples[-1], stats)\n",
        "        for i, graph_id in enumerate(graph_ids):\n",
        "            stat_x = stats[i].view(-1, args.n_condition).detach().cpu().numpy()\n",
        "            Gs_generated = construct_nx_from_adj(adj[i].detach().cpu().numpy())\n",
        "            edge_list_text = \", \".join([f\"({u}, {v})\" for u, v in Gs_generated.edges()])\n",
        "            writer.writerow([graph_id, edge_list_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-14T13:24:01.061491Z",
          "iopub.status.busy": "2025-01-14T13:24:01.061118Z",
          "iopub.status.idle": "2025-01-14T13:24:02.315154Z",
          "shell.execute_reply": "2025-01-14T13:24:02.313913Z",
          "shell.execute_reply.started": "2025-01-14T13:24:01.061458Z"
        },
        "id": "7TuTv_LIjjLc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Compute MAE for validation set\n",
        "MAE_val = 0\n",
        "for data in tqdm(val_loader, desc='Processing validation set'):\n",
        "    data = data.to(device)\n",
        "    graph_ids, stats = data.filename, data.stats\n",
        "    samples = sample(denoise_model, stats, latent_dim=args.latent_dim, timesteps=args.timesteps, betas=betas, batch_size=stats.size(0))\n",
        "    adj = autoencoder.decode_mu(samples[-1])\n",
        "\n",
        "    for i, graph_id in enumerate(graph_ids):\n",
        "        Gs_generated = construct_nx_from_adj(adj[i].detach().cpu().numpy())\n",
        "\n",
        "        if i % 500 == 0 :\n",
        "            print('stats gt :', np.round(stats[i].detach().cpu().numpy(), 2))\n",
        "            print('stats pr :', np.round(graph_features(Gs_generated.edges()), 2))\n",
        "\n",
        "        MAE_val += np.sum(np.abs(stat_x - graph_features(Gs_generated.edges())))\n",
        "\n",
        "MAE_val /= len(val_loader.dataset)\n",
        "print('MAE validation set:', MAE_val)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-14T13:03:00.134542Z",
          "iopub.status.idle": "2025-01-14T13:03:00.134795Z",
          "shell.execute_reply": "2025-01-14T13:03:00.134690Z"
        },
        "id": "1DxdtjYvjjLc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Compute MAE for training set\n",
        "MAE_train = 0\n",
        "for data in tqdm(train_loader, desc='Processing training set'):\n",
        "    data = data.to(device)\n",
        "    graph_ids, stats = data.filename, data.stats\n",
        "    samples = sample(denoise_model, stats, latent_dim=args.latent_dim, timesteps=args.timesteps, betas=betas, batch_size=stats.size(0))\n",
        "    adj = autoencoder.decode_mu(samples[-1])\n",
        "\n",
        "    for i, graph_id in enumerate(graph_ids):\n",
        "        Gs_generated = construct_nx_from_adj(adj[i].detach().cpu().numpy())\n",
        "        if i % 4000 == 0 :\n",
        "            print('stats gt :', np.round(stats[i].detach().cpu().numpy(), 2))\n",
        "            print('stats pr :', np.round(graph_features(Gs_generated.edges()), 2))\n",
        "\n",
        "        MAE_train += np.sum(np.abs(stat_x - graph_features(Gs_generated.edges())))\n",
        "MAE_train /= len(train_loader.dataset)\n",
        "print('MAE train set:', MAE_train)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30823,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
